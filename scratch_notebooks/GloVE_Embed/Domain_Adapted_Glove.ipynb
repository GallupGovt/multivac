{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load in Cleaned Article Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load in in the parsed article files in .json format and output as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "\n",
    "os.chdir('/Users/taylor_bolt/PycharmProjects/multivac/multivac')\n",
    "\n",
    "with open(\"data/output_20195727_095737.json\", 'r', encoding='utf-8') as jf:\n",
    "    src_data = json.load(jf)\n",
    " \n",
    "texts = [art[list(art.keys())[0]]['text'] for art in src_data]\n",
    " \n",
    "# The \"unidecode\" step simplifies non-ASCII chars which\n",
    "# mess up the R GloVe engine. Probably a more sophisticated way to\n",
    "# bridge that gap but this is the quick and dirty solution\n",
    " \n",
    "texts_df = pd.Series(texts).apply(lambda x: unidecode(x))\n",
    "texts_df = pd.DataFrame({'text':texts_df})\n",
    "del texts, src_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Domain-Specific GloVe Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create the domain-specific (DS) GloVe embedding model from the parsed journal article text. This is run from the 'trainEmbeddings.R' script, but called from Python using rpy2. This takes a while, so for quickness, we load in a previously run script. Given the size of the vocabulary, this process takes a lot of memory usage. We recommend that this process is run on a computer with at least 16GB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required package: data.table\n",
      "\n",
      "R[write to console]: Loading required package: dplyr\n",
      "\n",
      "R[write to console]: \n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:data.table’:\n",
      "\n",
      "    between, first, last\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "R[write to console]: Loading required package: text2vec\n",
      "\n",
      "R[write to console]: Loading required package: Rtsne\n",
      "\n",
      "R[write to console]: Loading required package: quanteda\n",
      "\n",
      "R[write to console]: Package version: 1.4.1\n",
      "\n",
      "R[write to console]: Parallel computing: 2 of 12 threads used.\n",
      "\n",
      "R[write to console]: See https://quanteda.io for tutorials and examples.\n",
      "\n",
      "R[write to console]: \n",
      "Attaching package: ‘quanteda’\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from ‘package:utils’:\n",
      "\n",
      "    View\n",
      "\n",
      "\n",
      "R[write to console]: Loading required package: doParallel\n",
      "\n",
      "R[write to console]: Loading required package: foreach\n",
      "\n",
      "R[write to console]: Loading required package: iterators\n",
      "\n",
      "R[write to console]: Loading required package: parallel\n",
      "\n",
      "R[write to console]: kept 12,075 features\n",
      "R[write to console]: \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO [2019-10-09 13:07:24] 2019-10-09 13:07:24 - epoch 1, expected cost 0.0563\n",
      "INFO [2019-10-09 13:07:28] 2019-10-09 13:07:28 - epoch 2, expected cost 0.0294\n",
      "INFO [2019-10-09 13:07:32] 2019-10-09 13:07:32 - epoch 3, expected cost 0.0226\n",
      "INFO [2019-10-09 13:07:36] 2019-10-09 13:07:36 - epoch 4, expected cost 0.0191\n",
      "INFO [2019-10-09 13:07:39] 2019-10-09 13:07:39 - epoch 5, expected cost 0.0168\n",
      "INFO [2019-10-09 13:07:43] 2019-10-09 13:07:43 - epoch 6, expected cost 0.0152\n",
      "INFO [2019-10-09 13:07:47] 2019-10-09 13:07:47 - epoch 7, expected cost 0.0139\n",
      "INFO [2019-10-09 13:07:51] 2019-10-09 13:07:51 - epoch 8, expected cost 0.0129\n",
      "INFO [2019-10-09 13:07:54] 2019-10-09 13:07:54 - epoch 9, expected cost 0.0121\n",
      "INFO [2019-10-09 13:07:58] 2019-10-09 13:07:58 - epoch 10, expected cost 0.0115\n",
      "INFO [2019-10-09 13:08:02] 2019-10-09 13:08:02 - epoch 11, expected cost 0.0109\n",
      "INFO [2019-10-09 13:08:06] 2019-10-09 13:08:06 - epoch 12, expected cost 0.0104\n",
      "INFO [2019-10-09 13:08:09] 2019-10-09 13:08:09 - epoch 13, expected cost 0.0099\n",
      "INFO [2019-10-09 13:08:13] 2019-10-09 13:08:13 - epoch 14, expected cost 0.0096\n",
      "INFO [2019-10-09 13:08:17] 2019-10-09 13:08:17 - epoch 15, expected cost 0.0092\n",
      "INFO [2019-10-09 13:08:21] 2019-10-09 13:08:21 - epoch 16, expected cost 0.0089\n",
      "INFO [2019-10-09 13:08:24] 2019-10-09 13:08:24 - epoch 17, expected cost 0.0086\n",
      "INFO [2019-10-09 13:08:28] 2019-10-09 13:08:28 - epoch 18, expected cost 0.0084\n",
      "INFO [2019-10-09 13:08:32] 2019-10-09 13:08:32 - epoch 19, expected cost 0.0081\n",
      "INFO [2019-10-09 13:08:35] 2019-10-09 13:08:35 - epoch 20, expected cost 0.0079\n",
      "INFO [2019-10-09 13:08:39] 2019-10-09 13:08:39 - epoch 21, expected cost 0.0077\n",
      "INFO [2019-10-09 13:08:43] 2019-10-09 13:08:43 - epoch 22, expected cost 0.0075\n",
      "INFO [2019-10-09 13:08:47] 2019-10-09 13:08:47 - epoch 23, expected cost 0.0073\n",
      "INFO [2019-10-09 13:08:50] 2019-10-09 13:08:50 - epoch 24, expected cost 0.0072\n",
      "INFO [2019-10-09 13:08:54] 2019-10-09 13:08:54 - epoch 25, expected cost 0.0070\n",
      "INFO [2019-10-09 13:08:58] 2019-10-09 13:08:58 - epoch 26, expected cost 0.0069\n",
      "INFO [2019-10-09 13:09:02] 2019-10-09 13:09:02 - epoch 27, expected cost 0.0068\n",
      "INFO [2019-10-09 13:09:06] 2019-10-09 13:09:06 - epoch 28, expected cost 0.0066\n",
      "INFO [2019-10-09 13:09:10] 2019-10-09 13:09:10 - epoch 29, expected cost 0.0065\n",
      "INFO [2019-10-09 13:09:14] 2019-10-09 13:09:14 - epoch 30, expected cost 0.0064\n",
      "INFO [2019-10-09 13:09:17] 2019-10-09 13:09:17 - epoch 31, expected cost 0.0063\n",
      "INFO [2019-10-09 13:09:21] 2019-10-09 13:09:21 - epoch 32, expected cost 0.0062\n",
      "INFO [2019-10-09 13:09:25] 2019-10-09 13:09:25 - epoch 33, expected cost 0.0061\n",
      "INFO [2019-10-09 13:09:29] 2019-10-09 13:09:29 - epoch 34, expected cost 0.0060\n",
      "INFO [2019-10-09 13:09:32] 2019-10-09 13:09:32 - epoch 35, expected cost 0.0059\n",
      "INFO [2019-10-09 13:09:36] 2019-10-09 13:09:36 - epoch 36, expected cost 0.0058\n",
      "INFO [2019-10-09 13:09:40] 2019-10-09 13:09:40 - epoch 37, expected cost 0.0057\n",
      "INFO [2019-10-09 13:09:44] 2019-10-09 13:09:44 - epoch 38, expected cost 0.0056\n",
      "INFO [2019-10-09 13:09:48] 2019-10-09 13:09:48 - epoch 39, expected cost 0.0056\n",
      "INFO [2019-10-09 13:09:51] 2019-10-09 13:09:51 - epoch 40, expected cost 0.0055\n",
      "INFO [2019-10-09 13:09:55] 2019-10-09 13:09:55 - epoch 41, expected cost 0.0054\n",
      "INFO [2019-10-09 13:09:59] 2019-10-09 13:09:59 - epoch 42, expected cost 0.0053\n",
      "INFO [2019-10-09 13:10:02] 2019-10-09 13:10:02 - epoch 43, expected cost 0.0053\n",
      "INFO [2019-10-09 13:10:06] 2019-10-09 13:10:06 - epoch 44, expected cost 0.0052\n",
      "INFO [2019-10-09 13:10:10] 2019-10-09 13:10:10 - epoch 45, expected cost 0.0052\n",
      "INFO [2019-10-09 13:10:13] 2019-10-09 13:10:13 - epoch 46, expected cost 0.0051\n",
      "INFO [2019-10-09 13:10:17] 2019-10-09 13:10:17 - epoch 47, expected cost 0.0050\n",
      "INFO [2019-10-09 13:10:21] 2019-10-09 13:10:21 - epoch 48, expected cost 0.0050\n",
      "INFO [2019-10-09 13:10:24] 2019-10-09 13:10:24 - epoch 49, expected cost 0.0049\n",
      "INFO [2019-10-09 13:10:28] 2019-10-09 13:10:28 - epoch 50, expected cost 0.0049\n",
      "INFO [2019-10-09 13:10:32] 2019-10-09 13:10:32 - epoch 51, expected cost 0.0048\n",
      "INFO [2019-10-09 13:10:36] 2019-10-09 13:10:36 - epoch 52, expected cost 0.0048\n",
      "INFO [2019-10-09 13:10:39] 2019-10-09 13:10:39 - epoch 53, expected cost 0.0047\n",
      "INFO [2019-10-09 13:10:43] 2019-10-09 13:10:43 - epoch 54, expected cost 0.0047\n",
      "INFO [2019-10-09 13:10:43] Success: early stopping. Improvement at iterartion 54 is less then convergence_tol\n"
     ]
    }
   ],
   "source": [
    "from rpy2.robjects import r, pandas2ri, numpy2ri\n",
    "\n",
    "# Source all the functions contained in the 'trainEmbeddings' R file\n",
    "r(\"source('src/data/trainEmbeddings.R')\")\n",
    "# Call the main GloveEmbedding function from the R script\n",
    "trainEmbeddings_R = r(\"trainEmbeddings\")\n",
    "# Train DS GloVe Embedding model and ouput as a Numpy Matrix\n",
    "pandas2ri.activate()\n",
    "DS_embeddings_R = trainEmbeddings_R(texts_df)\n",
    "del texts_df\n",
    "pandas2ri.deactivate()\n",
    "DS_embeddings = numpy2ri.rpy2py(DS_embeddings_R[0])\n",
    "# Get DS GloVe vocabulary\n",
    "domain_spec_vocab = list(DS_embeddings_R[1])\n",
    "del DS_embeddings_R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Import Domain-General GloVe Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load in the domain-general (DG) GloVe embedding model from the 'Common Crawl' pre-trained model from Stanford (https://nlp.stanford.edu/projects/glove/). This is saved as a .txt on disk. This takes a while to load (~30min). Thus, repeatedly loading this in is fairly inefficient. Rather, I have previously run this script and saved the relevant parts of the model to a pickle file on disk. However, I provide the code commented out below to load in the data if necessary (most relevant is the loadGloveModel function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "## Load in Stanford's 'Common Crawl' Domain-General Glove Embedding Model\n",
    "# Only pull out the words that are contained in our corpus\n",
    "# * This can take a while (~30min) - could use some optimization * \n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        if word in domain_spec_vocab:\n",
    "#             print(word)\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            model[word] = embedding\n",
    "    return model\n",
    "DG_embeddings = loadGloveModel('data/glove.42B.300d.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run CCA b/w Domain-General and Domain-Specific GloVe Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create the domain-adapted (DA) GloVe embeddings the CCA between on the tokens that are shared in common between the DG and DS vocabulary. The vectors for each token of the DA glove embedding model are derived from a weighted average of the canonical vectors (N = 100) from the CCA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11075, 300)\n",
      "(11075, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taylor_bolt/PycharmProjects/multivac/venv/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py:79: ConvergenceWarning: Maximum number of iterations reached\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import zscore \n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "## Processing to ensure rows match between the DG and DS embeddings\n",
    "# Convert domain general (DG) embedding from dictionary to array\n",
    "domain_gen_vocab = [token for token in DG_embeddings.keys()]\n",
    "DG_embeddings = np.array([DG_embeddings[i] for i in DG_embeddings.keys()])\n",
    "# Find the indices of matching words\n",
    "both = set(domain_gen_vocab).intersection(domain_spec_vocab)\n",
    "indices_gen = [domain_gen_vocab.index(x) for x in both]\n",
    "indices_spec = [domain_spec_vocab.index(x) for x in both]\n",
    "indices_spec_notDG = [domain_spec_vocab.index(x) for x in domain_spec_vocab if x not in both]\n",
    "\n",
    "# Sort and subset domain specific (DS) array to match indices of DG array\n",
    "DS_embeddings_subset = DS_embeddings[indices_spec,:].copy()\n",
    "DG_embeddings_subset = DG_embeddings[indices_gen,:].copy()\n",
    "\n",
    "\n",
    "def domain_adapted_CCA(DG_embed,DS_embed,NC=100):\n",
    "    # Z-score\n",
    "    DG_embed_norm = zscore(DG_embed)\n",
    "    print(DG_embed_norm.shape)\n",
    "    DS_embed_norm = zscore(DS_embed)\n",
    "    print(DS_embed_norm.shape)\n",
    "    # Initialize CCA Model\n",
    "    cca = CCA(n_components=NC)\n",
    "    cca.fit(DG_embed_norm,DS_embed_norm)\n",
    "    \n",
    "    DA_embeddings = (cca.x_scores_ + cca.y_scores_)/2\n",
    "    return cca, DA_embeddings\n",
    "\n",
    "cca_res, DA_embeddings = domain_adapted_CCA(DG_embeddings_subset,DS_embeddings_subset,NC=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Project Left-Out Domain-Specific Tokens on to Domain-Adapted Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The tokens of the DS embedding model that are left out of the intersection between the DS and DG embedding model are projected into the 100-dimensional canonical vector space from the CCA analysis (via matrix multiplication) and appended to the DA embedding vectors (created above). Your final output is 'DA_embeddings_final'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_embeddings_notinDG = DS_embeddings[indices_spec_notDG,:]\n",
    "DS_embeddings_notinDG_norm = zscore(DS_embeddings_notinDG)\n",
    "\n",
    "DA_notinDG_embeddings = cca_res.y_weights_.T @ DS_embeddings_notinDG_norm.T\n",
    "DA_embeddings_final = np.append(DA_embeddings,DA_notinDG_embeddings.T,axis=0)\n",
    "\n",
    "vocab_final_indx = indices_spec + indices_spec_notDG\n",
    "vocab_final = [domain_spec_vocab[i] for i in vocab_final_indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "glove_embeddings = {'embeddings': DA_embeddings_final, 'vocab': vocab_final}\n",
    "pickle.dump(glove_embeddings, open('data/DA_glove_embeddings.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
